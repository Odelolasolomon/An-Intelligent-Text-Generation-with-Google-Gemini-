{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an LLM application(RAG) using Langchain, (Google Gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv(\"Get_Api_Key\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Ask The Questions Using Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733399896.981744     195 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate_content(\u001b[39m\"\u001b[39;49m\u001b[39mExplain Generative AI with 3 bullet points\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39mtext)\n\u001b[1;32m      3\u001b[0m Markdown(response\u001b[39m.\u001b[39mtext)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py:262\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[39mreturn\u001b[39;00m generation_types\u001b[39m.\u001b[39mGenerateContentResponse\u001b[39m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    261\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49mgenerate_content(\n\u001b[1;32m    263\u001b[0m             request,\n\u001b[1;32m    264\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_options,\n\u001b[1;32m    265\u001b[0m         )\n\u001b[1;32m    266\u001b[0m         \u001b[39mreturn\u001b[39;00m generation_types\u001b[39m.\u001b[39mGenerateContentResponse\u001b[39m.\u001b[39mfrom_response(response)\n\u001b[1;32m    267\u001b[0m \u001b[39mexcept\u001b[39;00m google\u001b[39m.\u001b[39mapi_core\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mInvalidArgument \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:791\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    790\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[1;32m    792\u001b[0m     request,\n\u001b[1;32m    793\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    795\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    796\u001b[0m )\n\u001b[1;32m    798\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    294\u001b[0m     target,\n\u001b[1;32m    295\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[1;32m    296\u001b[0m     sleep_generator,\n\u001b[1;32m    297\u001b[0m     timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[1;32m    298\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[39m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[1;32m    154\u001b[0m         exc,\n\u001b[1;32m    155\u001b[0m         deadline,\n\u001b[1;32m    156\u001b[0m         sleep,\n\u001b[1;32m    157\u001b[0m         error_list,\n\u001b[1;32m    158\u001b[0m         predicate,\n\u001b[1;32m    159\u001b[0m         on_error,\n\u001b[1;32m    160\u001b[0m         exception_factory,\n\u001b[1;32m    161\u001b[0m         timeout,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[39m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[39m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[39m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mraise\u001b[39;00m final_exc \u001b[39mfrom\u001b[39;00m \u001b[39msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m on_error_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[39m=\u001b[39m target()\n\u001b[1;32m    145\u001b[0m         \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout \u001b[39m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 API key not valid. Please pass a valid API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key not valid. Please pass a valid API key.\"\n]"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Explain Generative AI with 3 bullet points\")\n",
    "print(response.text)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Chat With Gemini And Retrieve The Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.start_chat()\n",
    "response = hist.send_message(\"Hi! Give me a recipe to make a margeritta pizza from scratch.\")\n",
    "Markdown(response.text)\n",
    "\n",
    "for i in hist.history:\n",
    "    print(i)\n",
    "    print('\\n\\n')\n",
    "i.parts[0].text \n",
    "\n",
    "model.count_tokens(\"Now please help me find the nearest supermarket from where I can buy the ingrediants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Experiment With The Temperature Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for temp in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "  config = genai.types.GenerationConfig(temperature=temp)\n",
    "  result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "  print(f\"\\n\\nFor temperature value {temp}, the results are: \\n\\n\")\n",
    "  display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Experiment With Maximum Output Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, generation_config=generation_config)\n",
    "    return response\n",
    "for m_o_tok in [1, 50, 100, 150, 200]:\n",
    "    config = genai.types.GenerationConfig(max_output_tokens=m_o_tok)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor max output token value {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Experiment With the top_k Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for k in [1, 4, 16, 32, 40]:\n",
    "    config = genai.types.GenerationConfig(top_k=k)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor top k value {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Experiment With the top_p Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt, generation_config={}):\n",
    "    response = model.generate_content(contents=prompt, \n",
    "    generation_config=generation_config)\n",
    "    return response\n",
    "\n",
    "for p in [0, 0.2, 0.4, 0.8, 1]:\n",
    "    config = genai.types.GenerationConfig(top_p=p)\n",
    "    result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "\n",
    "    print(f\"\\n\\nFor top p value {temp}, the results are: \\n\\n\")\n",
    "    display(Markdown(result.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Experiment With the candidate_count Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = genai.types.GenerationConfig(candidate_count=1)\n",
    "result = get_response(\"Explain the concepts of XGBoost and Random Forest with real-life use cases\", generation_config=config)\n",
    "Markdown(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Introduction to Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the text-generation chatbot you are building, you’ll use retrieval-augmented generation (RAG), where a model extracts specific information to generate more relevant responses. The source of information can be a corpus, a document, a database, a website, or anything related.\n",
    "\n",
    "There are some strong advantages of using RAG, like:\n",
    "\n",
    "Enhanced contextual responses, increasing the LLM capabilities.\n",
    "\n",
    "Retrieval of up-to-date information since it retrieves information from the website, document, or corpus.\n",
    "\n",
    "Reduced hallucinations, which is a problem with the LLMs.\n",
    "\n",
    "LangChain’s modular design helps integrate various document loaders, embedding models, vector stores, and LLM providers. In the coming tasks, you’ll use the tools provided by LangChain in the order specified below:\n",
    "\n",
    "Load the PDF document (from where you will retrieve the specific data) using the PyPDFLoader.\n",
    "\n",
    "Extract the texts using RecursiveCharacterTextSplitter to have meaningful chunks of text for further processing.\n",
    "\n",
    "Use GoogleGenerativeAIEmbeddings to generate the embeddings for the extracted texts that will be used for similarity search in RAG.\n",
    "\n",
    "Use Chroma (a vector database) to store the created embeddings so that they can be used to retrieve the relevant information later when needed.\n",
    "\n",
    "Use the RetrievalQA function to build the question-answering system for retrieval (finding relevant context) with generation (generating an answer based on the retrieved context). This function will use ChatGoogleGenerativeAI with the Gemini Pro model to generate responses from the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Load the PDF and Extract the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 700\n",
    "CHUNK_OVERLAP = 100\n",
    "pdf_path = \"https://www.analytixlabs.co.in/assets/pdfs/Data_Engineering%20&_Other_Job_Roles-AnalytixLabs.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_path)\n",
    "split_pdf_document = pdf_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in split_pdf_document)\n",
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Create the Gemini Model and Create the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = ChatGoogleGenerativeAI(model='gemini-pro', google_api_key=<your_api_key>, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=<your_api_key>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Chroma.from_texts(texts, embeddings)\n",
    "retriever = vector_index.as_retriever(search_kwargs={\"k\" : 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Create the RAG Chain and Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(gemini_model, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage \n",
    "question = \"Which tools do Data Engineers primarily work with?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(\"Answer:\", result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
